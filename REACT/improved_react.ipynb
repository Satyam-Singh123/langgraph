{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb55476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processing Your Request ====\n",
      "\n",
      "\n",
      "==== Final Answer ====\n",
      "\n",
      "The tool call to find the best trending video on YouTube for children failed due to a connection error. Please try again later or check the API endpoint.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import ast\n",
    "import traceback\n",
    "\n",
    "# typing PyDantic\n",
    "from typing import Any, Optional, Dict, Callable\n",
    "\n",
    "# AgentState\n",
    "from typing import Annotated, List, TypedDict, Optional\n",
    "import operator\n",
    "\n",
    "# get .env \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Watsonx imports\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# LangGraph + messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "REACT_PROMPT = \"\"\"You are a specialized agent with access to multiple tools. Your goal is to answer the user's request using the available tools when needed.\n",
    "\n",
    "Available Tools:\n",
    "{tools_list}\n",
    "\n",
    "You must respond in one of two formats:\n",
    "\n",
    "1. Final Answer\n",
    "Think: [Your reasoning about why you have enough information]\n",
    "Action: Final Answer\n",
    "Action Input: [Your final response to the user]\n",
    "\n",
    "2. Tool Call\n",
    "Think: [Your reasoning about why you need to use a specific tool]\n",
    "Action: [tool_name]\n",
    "Action Input: [appropriate input parameters for the tool]\n",
    "\n",
    "Begin.\n",
    "\"\"\"\n",
    "\n",
    "def get_llm(model_id: str = \"meta-llama/llama-3-3-70b-instruct\",\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.2,\n",
    ") -> ModelInference:\n",
    "    \n",
    "    creds = Credentials(api_key=os.getenv(\"WATSONX_API_KEY\"), url=os.getenv(\"WATSONX_URL\"))\n",
    "    client = APIClient(credentials=creds)\n",
    "\n",
    "    return ModelInference(\n",
    "        model_id=model_id,\n",
    "        params={GenParams.MAX_NEW_TOKENS: max_new_tokens, GenParams.TEMPERATURE: temperature},\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT_ID\"), \n",
    "        api_client=client\n",
    "    )\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    final_output: Optional[str]\n",
    "    action_name: Optional[str]\n",
    "    action_input: Optional[str]\n",
    "\n",
    "def generate_transcript(topic: str, llm: Optional[ModelInference] = None) -> str:\n",
    "    \"\"\"Generate a YouTube transcript for kids on the given topic.\"\"\"\n",
    "    if llm is None:\n",
    "        llm = get_llm()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Generate a clear, spoken-style transcript for a YouTube trending video for kids about:\n",
    "\"{topic}\"\n",
    "\n",
    "Requirements:\n",
    "- 400-700 words\n",
    "- Conversational tone\n",
    "- Clear structure: hook ‚Üí points ‚Üí ending\n",
    "- No directions or metadata, only spoken words\n",
    "- Age-appropriate content for children\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.generate_text(prompt)\n",
    "\n",
    "    if isinstance(response, dict) and \"results\" in response:\n",
    "        return response[\"results\"][0].get(\"generated_text\", \"\")\n",
    "    elif isinstance(response, str):\n",
    "        return response\n",
    "    return str(response)\n",
    "\n",
    "def tavily_search(query: str, top_k: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Real Tavily search call.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    endpoint = os.getenv(\"TAVILY_ENDPOINT\")\n",
    "\n",
    "    if not api_key or not endpoint:\n",
    "        raise RuntimeError(\"Missing Tavily environment variables.\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\"query\": query, \"top_k\": top_k}\n",
    "    resp = requests.post(endpoint, headers=headers, json=payload, timeout=25)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    return resp.json()\n",
    "\n",
    "# Enhanced tools dictionary with descriptions\n",
    "TOOLS: Dict[str, Dict[str, Any]] = {\n",
    "    \"tavily_search\": {\n",
    "        \"function\": tavily_search,\n",
    "        \"description\": \"Searches the internet for current information on any topic. Use when you need up-to-date facts or recent information.\",\n",
    "        \"input_schema\": {\n",
    "            \"query\": \"string - the search query\",\n",
    "            \"top_k\": \"integer - number of results (default: 5)\"\n",
    "        }\n",
    "    },\n",
    "    \"generate_transcript\": {\n",
    "        \"function\": generate_transcript,\n",
    "        \"description\": \"Generates a YouTube-style transcript for kids on a given topic. Use when the user asks for content creation, scripts, or educational material for children.\",\n",
    "        \"input_schema\": {\n",
    "            \"topic\": \"string - the topic for the transcript\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def react_llm_node(state: AgentState):\n",
    "    \"\"\"Main reasoning node that decides which tool to use or provides final answer.\"\"\"\n",
    "    \n",
    "    # Build conversation history\n",
    "    conversation = \"\"\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, AIMessage):\n",
    "            conversation += f\"Assistant: {msg.content}\\n\"\n",
    "        elif isinstance(msg, HumanMessage):\n",
    "            conversation += f\"User: {msg.content}\\n\"\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            conversation += f\"Tool Result: {msg.content}\\n\"\n",
    "\n",
    "    # Build tools list for prompt\n",
    "    tools_list = \"\"\n",
    "    for tool_name, tool_info in TOOLS.items():\n",
    "        tools_list += f\"- {tool_name}: {tool_info['description']}\\n\"\n",
    "        tools_list += f\"  Input: {tool_info['input_schema']}\\n\\n\"\n",
    "\n",
    "    prompt = REACT_PROMPT.format(tools_list=tools_list) + \"\\n\" + conversation\n",
    "\n",
    "    llm = get_llm()\n",
    "    llm_result = llm.generate_text(prompt)\n",
    "    \n",
    "    if isinstance(llm_result, dict) and \"results\" in llm_result:\n",
    "        react_llm_result = llm_result[\"results\"][0][\"generated_text\"]\n",
    "    elif isinstance(llm_result, str):\n",
    "        react_llm_result = llm_result\n",
    "    else:\n",
    "        react_llm_result = str(llm_result)\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=react_llm_result)]}\n",
    "\n",
    "def react_llm_parser_and_decide_node(state: AgentState):\n",
    "    \"\"\"Parse the LLM response and decide next action.\"\"\"\n",
    "    \n",
    "    last = state[\"messages\"][-1]\n",
    "    text = last.content or \"\"\n",
    "    \n",
    "    # Look for action pattern\n",
    "    m = re.search(\n",
    "        r\"Action:\\s*(.+?)\\s*\\nAction Input:\\s*(.+)\",\n",
    "        text,\n",
    "        re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "\n",
    "    if not m:\n",
    "        # Check if it's a final answer without explicit action\n",
    "        if \"Final Answer\" in text or any(indicator in text.lower() for indicator in [\"here is\", \"the answer is\", \"i can tell you\"]):\n",
    "            # Extract the actual answer content\n",
    "            lines = text.split('\\n')\n",
    "            answer_lines = [line for line in lines if not line.startswith('Think:') and not line.startswith('Action:')]\n",
    "            final_answer = ' '.join(answer_lines).strip()\n",
    "            return {\"action_name\": None, \"action_input\": final_answer}\n",
    "        return {\"action_name\": None, \"action_input\": None}\n",
    "\n",
    "    action_name = m.group(1).strip()\n",
    "    action_input_raw = m.group(2).strip()\n",
    "\n",
    "    # Parse action input\n",
    "    try:\n",
    "        action_input = json.loads(action_input_raw)\n",
    "    except:\n",
    "        try:\n",
    "            action_input = ast.literal_eval(action_input_raw)\n",
    "        except:\n",
    "            # For simple string inputs, create appropriate structure\n",
    "            if action_name == \"generate_transcript\":\n",
    "                action_input = {\"topic\": action_input_raw}\n",
    "            elif action_name == \"tavily_search\":\n",
    "                action_input = {\"query\": action_input_raw}\n",
    "            else:\n",
    "                action_input = {\"raw\": action_input_raw}\n",
    "            \n",
    "    if action_name.lower() == \"final answer\":\n",
    "        return {\"action_name\": None, \"action_input\": action_input}\n",
    "    \n",
    "    return {\"action_name\": action_name, \"action_input\": action_input}\n",
    "\n",
    "def tool_executor_node(state: AgentState):\n",
    "    \"\"\"Execute the selected tool.\"\"\"\n",
    "    name = state.get(\"action_name\")\n",
    "    data = state.get(\"action_input\") or {}\n",
    "\n",
    "    if not name:\n",
    "        return {}\n",
    "\n",
    "    tool_info = TOOLS.get(name)\n",
    "    if not tool_info:\n",
    "        result_text = f\"Unknown tool: {name}\"\n",
    "    else:\n",
    "        try:\n",
    "            tool_fn = tool_info[\"function\"]\n",
    "            # Handle different tool input formats\n",
    "            if name == \"generate_transcript\":\n",
    "                if isinstance(data, dict) and \"topic\" in data:\n",
    "                    result = tool_fn(topic=data[\"topic\"])\n",
    "                elif isinstance(data, str):\n",
    "                    result = tool_fn(topic=data)\n",
    "                else:\n",
    "                    result = tool_fn(topic=str(data))\n",
    "            elif name == \"tavily_search\":\n",
    "                if isinstance(data, dict):\n",
    "                    result = tool_fn(**data)\n",
    "                else:\n",
    "                    result = tool_fn(query=str(data))\n",
    "            else:\n",
    "                result = tool_fn(**data) if isinstance(data, dict) else tool_fn(data)\n",
    "                \n",
    "            result_text = json.dumps(result, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            result_text = json.dumps({\"error\": str(e), \"trace\": traceback.format_exc()})\n",
    "\n",
    "    # Create tool message\n",
    "    tool_msg = ToolMessage(\n",
    "        content=result_text,\n",
    "        tool_call_id=name\n",
    "    )\n",
    "\n",
    "    return {\"messages\": [tool_msg], \"action_name\": None, \"action_input\": None}\n",
    "\n",
    "def extract_final_answer(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Extract and return the final answer from the conversation.\"\"\"\n",
    "    # Look for the last AI message that contains a final answer\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            content = msg.content or \"\"\n",
    "            # If it explicitly says \"Final Answer\"\n",
    "            if \"Final Answer\" in content:\n",
    "                m = re.search(r\"Action Input:\\s*(.+)$\", content, re.S)\n",
    "                if m:\n",
    "                    return {\"final_output\": m.group(1).strip()}\n",
    "                # Try to extract the answer after \"Final Answer\"\n",
    "                lines = content.split('\\n')\n",
    "                for i, line in enumerate(lines):\n",
    "                    if \"Final Answer\" in line and i + 1 < len(lines):\n",
    "                        return {\"final_output\": lines[i + 1].strip()}\n",
    "                return {\"final_output\": content}\n",
    "            \n",
    "            # If this is the last message and no tool calls were made, it might be the answer\n",
    "            if msg == state[\"messages\"][-1] and not any(isinstance(m, ToolMessage) for m in state[\"messages\"][state[\"messages\"].index(msg):]):\n",
    "                return {\"final_output\": content}\n",
    "    \n",
    "    # Check if we have a final answer from the router\n",
    "    if state.get(\"action_name\") is None and state.get(\"action_input\"):\n",
    "        return {\"final_output\": state[\"action_input\"]}\n",
    "        \n",
    "    return {\"final_output\": None}\n",
    "\n",
    "def build_graph():\n",
    "    \"\"\"Build the LangGraph workflow.\"\"\"\n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # Add nodes\n",
    "    graph.add_node(\"react_llm\", react_llm_node)\n",
    "    graph.add_node(\"router\", react_llm_parser_and_decide_node)\n",
    "    graph.add_node(\"tool_executor\", tool_executor_node)\n",
    "    graph.add_node(\"extract_final_answer\", extract_final_answer)\n",
    "\n",
    "    # Build workflow\n",
    "    graph.add_edge(START, \"react_llm\")\n",
    "    graph.add_edge(\"react_llm\", \"router\")\n",
    "\n",
    "    def route_fn(state: AgentState):\n",
    "        \"\"\"Route to final answer or tool execution.\"\"\"\n",
    "        return \"extract_final_answer\" if state.get(\"action_name\") is None else \"tool_executor\"\n",
    "\n",
    "    graph.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_fn,\n",
    "        {\n",
    "            \"extract_final_answer\": \"extract_final_answer\", \n",
    "            \"tool_executor\": \"tool_executor\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    graph.add_edge(\"extract_final_answer\", END)\n",
    "    graph.add_edge(\"tool_executor\", \"react_llm\")  # Loop back for multi-step reasoning\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    graph = build_graph()\n",
    "    app = graph.compile()\n",
    "\n",
    "    user_query = input(\"Enter your query/topic: \").strip()\n",
    "\n",
    "    state: AgentState = {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"action_name\": None,\n",
    "        \"action_input\": None,\n",
    "        \"final_output\": None,\n",
    "    }\n",
    "\n",
    "    print(\"\\n==== Processing Your Request ====\\n\")\n",
    "    \n",
    "    # Run the graph\n",
    "    result = app.invoke(state)\n",
    "    \n",
    "    final_output = result.get(\"final_output\")\n",
    "    \n",
    "    if final_output:\n",
    "        print(\"\\n==== Final Answer ====\\n\")\n",
    "        print(final_output)\n",
    "    else:\n",
    "        print(\"\\n==== No Clear Final Answer Found ====\\n\")\n",
    "        print(\"Conversation history:\")\n",
    "        for i, m in enumerate(result[\"messages\"]):\n",
    "            print(f\"\\n--- Step {i} ({type(m).__name__}) ---\")\n",
    "            print(m.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d601ced8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Search for what is trending in AI today and then write a kids transcript about it.\n",
      "\n",
      "\n",
      "--- Node: agent ---\n",
      "üõ†Ô∏è  Thinking: I need to use tools: ['tavily_search']\n",
      "\n",
      "--- Node: tools ---\n",
      "‚ö° Tool Output: Search results for trending in AI today: 1. WatsonX is an enterprise AI platform......\n",
      "\n",
      "--- Node: agent ---\n",
      "üõ†Ô∏è  Thinking: I need to use tools: ['generate_transcript']\n",
      "\n",
      "--- Node: tools ---\n",
      "‚ö° Tool Output: Hey kids! Welcome back! Today we are talking about WatsonX is an enterprise AI platform......\n",
      "\n",
      "--- Node: agent ---\n",
      "ü§ñ Agent: The current trend in AI is WatsonX, an enterprise AI platform. Here's a kids' transcript about it: \"Hey kids! Welcome back! Today we are talking about WatsonX is an enterprise AI platform...\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. The Modern Imports\n",
    "from langchain_ibm import ChatWatsonx\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode # <--- The professional way to run tools\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- SECTION 1: DEFINE TOOLS (The Right Way) ---\n",
    "# Using @tool decorators automatically generates the JSON schema. \n",
    "# No manual dicts required.\n",
    "\n",
    "@tool\n",
    "def tavily_search(query: str):\n",
    "    \"\"\"\n",
    "    Perform a web search for up-to-date information.\n",
    "    Use this when you need current events or facts.\n",
    "    \"\"\"\n",
    "    # Mocking the actual API call for the demo (replace with your requests logic)\n",
    "    return f\"Search results for {query}: 1. WatsonX is an enterprise AI platform...\"\n",
    "\n",
    "@tool\n",
    "def generate_transcript(topic: str):\n",
    "    \"\"\"\n",
    "    Generates a YouTube-style transcript for kids on a given topic.\n",
    "    \"\"\"\n",
    "    return f\"Hey kids! Welcome back! Today we are talking about {topic}...\"\n",
    "\n",
    "# List of tools\n",
    "tools = [tavily_search, generate_transcript]\n",
    "\n",
    "# --- SECTION 2: THE BRAIN (Model Binding) ---\n",
    "\n",
    "# We use ChatWatsonx because it handles the \"Chat\" structure natively.\n",
    "# Replace \"model_id\" with your specific Llama 3 version available in WatsonX\n",
    "llm = ChatWatsonx(\n",
    "    model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "    url=os.getenv(\"WATSONX_URL\"),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    params={\n",
    "        \"decoding_method\": \"greedy\",\n",
    "        \"max_new_tokens\": 500\n",
    "    }\n",
    ")\n",
    "\n",
    "# BINDING: This is the key. We attach tools to the LLM signature.\n",
    "# The LLM now \"knows\" these functions exist and will output specific structures to call them.\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# --- SECTION 3: STATE & NODES ---\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # add_messages handles the history automatically. No manual string concatenation.\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "\n",
    "def reasoner_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    The Agent Node. It simply looks at history and decides what to do.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# We do not need a \"parser\" node. The ToolNode handles execution automatically.\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# --- SECTION 4: CONTROL FLOW (The Router) ---\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Decide if we loop or stop.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the LLM returned a tool_call, we MUST go to the tools node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    # Otherwise, we are done\n",
    "    return \"__end__\"\n",
    "\n",
    "# --- SECTION 5: THE GRAPH ARCHITECTURE ---\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"agent\", reasoner_node)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Add Edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Conditional Edge: This replaces your complex \"router node\"\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# The Cycle: After tools run, feedback into the agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "# --- SECTION 6: EXECUTION ---\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def reasoner_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    The Agent Node with Self-Correction Logic.\n",
    "    \"\"\"\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    \n",
    "    # --- RESILIENCE BLOCK ---\n",
    "    # Check if the model tried to call a tool but the parser missed it\n",
    "    if not response.tool_calls and response.content:\n",
    "        content = response.content\n",
    "        if isinstance(content, str) and content.strip().startswith(\"[\") and \"type\" in content:\n",
    "            try:\n",
    "                # 1. Attempt to parse the content as a list\n",
    "                # The model output came as a string representing a list of strings\n",
    "                parsed_list = json.loads(content)\n",
    "                \n",
    "                tool_calls = []\n",
    "                for item in parsed_list:\n",
    "                    # 2. If the item is a string (double-encoded JSON), parse it again\n",
    "                    if isinstance(item, str):\n",
    "                        item = json.loads(item)\n",
    "                    \n",
    "                    # 3. Standardize to LangChain ToolCall format\n",
    "                    if item.get(\"type\") == \"function\" or \"name\" in item:\n",
    "                        # Extract name and args depending on format\n",
    "                        func_name = item.get(\"name\") or item.get(\"function\", {}).get(\"name\")\n",
    "                        func_args = item.get(\"parameters\") or item.get(\"function\", {}).get(\"arguments\") or {}\n",
    "                        \n",
    "                        if func_name:\n",
    "                            tool_calls.append({\n",
    "                                \"name\": func_name,\n",
    "                                \"args\": func_args,\n",
    "                                \"id\": f\"call_{uuid.uuid4()}\" # Required by LangGraph\n",
    "                            })\n",
    "                \n",
    "                if tool_calls:\n",
    "                    print(f\"‚úÖ SYSTEM: Auto-corrected {len(tool_calls)} raw tool calls.\")\n",
    "                    response.tool_calls = tool_calls\n",
    "                    # Clear content so we don't re-process it or confuse the next step\n",
    "                    response.content = \"\" \n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                pass # It was just normal text, ignore\n",
    "    # ------------------------\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n",
    "# 1. Add ToolMessage and AIMessage to your imports\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, AIMessage\n",
    "\n",
    "# 2. Replace the main() function with this type-safe version\n",
    "def main():\n",
    "    user_query = \"Search for what is trending in AI today and then write a kids transcript about it.\"\n",
    "    print(f\"User: {user_query}\\n\")\n",
    "    \n",
    "    inputs = {\"messages\": [HumanMessage(content=user_query)]}\n",
    "    \n",
    "    try:\n",
    "        for event in app.stream(inputs, config={\"recursion_limit\": 10}):\n",
    "            for key, value in event.items():\n",
    "                print(f\"\\n--- Node: {key} ---\")\n",
    "                \n",
    "                # Get the last message generated by this node\n",
    "                last_msg = value[\"messages\"][-1]\n",
    "                \n",
    "                # --- TYPE SAFETY CHECK ---\n",
    "                \n",
    "                # CASE 1: The LLM (Agent) is speaking\n",
    "                if isinstance(last_msg, AIMessage):\n",
    "                    if last_msg.tool_calls:\n",
    "                        print(f\"üõ†Ô∏è  Thinking: I need to use tools: {[t['name'] for t in last_msg.tool_calls]}\")\n",
    "                    else:\n",
    "                        print(f\"ü§ñ Agent: {last_msg.content}\")\n",
    "                \n",
    "                # CASE 2: The Tool is responding\n",
    "                elif isinstance(last_msg, ToolMessage):\n",
    "                    print(f\"‚ö° Tool Output: {last_msg.content[:100]}...\") # Truncate long outputs\n",
    "                    \n",
    "                # CASE 3: Fallback\n",
    "                else:\n",
    "                    print(f\"Unknown Message: {last_msg}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035be528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import traceback\n",
    "from typing import Annotated, Literal, TypedDict, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- LangChain / LangGraph Imports ---\n",
    "from langchain_ibm import ChatWatsonx\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver  # <--- The Memory Component\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 1: THE TOOLS (The Hands)\n",
    "# ==========================================\n",
    "\n",
    "@tool\n",
    "def tavily_search(query: str):\n",
    "    \"\"\"\n",
    "    Perform a web search for up-to-date information. \n",
    "    Use this when you need current events, facts, or trending topics.\n",
    "    \"\"\"\n",
    "    # In a real scenario, you would call the Tavily API here.\n",
    "    # For this demo, we mock the return to ensure it runs without extra keys.\n",
    "    print(f\"    (System: Searching internet for '{query}'...)\")\n",
    "    return f\"Search Results for '{query}': 1. WatsonX is trending for its enterprise safety. 2. Agentic workflows are the new standard in 2025. 3. Python 3.14 was just released.\"\n",
    "\n",
    "@tool\n",
    "def generate_transcript(topic: str):\n",
    "    \"\"\"\n",
    "    Generates a YouTube-style transcript for kids on a given topic.\n",
    "    \"\"\"\n",
    "    print(f\"    (System: Generating content for '{topic}'...)\")\n",
    "    return f\"Hey there Super Kids! Welcome back to the channel! Today we are learning about {topic}! It's like a robot brain that helps people do work faster!\"\n",
    "\n",
    "tools = [tavily_search, generate_transcript]\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 2: THE MODEL (The Brain)\n",
    "# ==========================================\n",
    "\n",
    "# We use Llama 3.3 70B Instruct as it is the most capable model available in your env.\n",
    "llm = ChatWatsonx(\n",
    "    model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "    url=os.getenv(\"WATSONX_URL\"),\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    params={\n",
    "        \"decoding_method\": \"greedy\",\n",
    "        \"max_new_tokens\": 1000,\n",
    "        \"temperature\": 0.1, # Keep temp low for precise tool calling\n",
    "    }\n",
    ")\n",
    "\n",
    "# Bind the tools to the LLM so it knows they exist\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 3: STATE & LOGIC (The Nervous System)\n",
    "# ==========================================\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # 'add_messages' ensures we append to history rather than overwriting it\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "def reasoner_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    The main reasoning node. Includes a resilience pattern to fix \n",
    "    malformed JSON if the model outputs a string instead of a structured object.\n",
    "    \"\"\"\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    \n",
    "    # --- RESILIENCE BLOCK (Auto-Correction) ---\n",
    "    # Sometimes models return a string looking like JSON instead of actual tool calls.\n",
    "    # We catch that here and fix it manually.\n",
    "    if not response.tool_calls and response.content:\n",
    "        content = response.content\n",
    "        if isinstance(content, str) and content.strip().startswith(\"[\") and \"type\" in content:\n",
    "            try:\n",
    "                parsed_list = json.loads(content)\n",
    "                tool_calls = []\n",
    "                for item in parsed_list:\n",
    "                    if isinstance(item, str): item = json.loads(item)\n",
    "                    \n",
    "                    # Extract standard fields\n",
    "                    func_name = item.get(\"name\") or item.get(\"function\", {}).get(\"name\")\n",
    "                    func_args = item.get(\"parameters\") or item.get(\"function\", {}).get(\"arguments\") or {}\n",
    "                    \n",
    "                    if func_name:\n",
    "                        tool_calls.append({\n",
    "                            \"name\": func_name,\n",
    "                            \"args\": func_args,\n",
    "                            \"id\": f\"call_{uuid.uuid4()}\"\n",
    "                        })\n",
    "                \n",
    "                if tool_calls:\n",
    "                    print(f\"    (System: Auto-corrected {len(tool_calls)} raw tool calls)\")\n",
    "                    response.tool_calls = tool_calls\n",
    "                    response.content = \"\" # Clear text so we don't confuse the next step\n",
    "            except Exception:\n",
    "                pass # If parsing fails, just let it be normal text\n",
    "    # ------------------------------------------\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Router Logic: If the agent wants to call a tool, go to 'tools'.\n",
    "    Otherwise, stop.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 4: THE GRAPH (The Architecture)\n",
    "# ==========================================\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 1. Add Nodes\n",
    "workflow.add_node(\"agent\", reasoner_node)\n",
    "workflow.add_node(\"tools\", ToolNode(tools)) # Using LangGraph's prebuilt optimized ToolNode\n",
    "\n",
    "# 2. Add Edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# 3. Conditional Edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# 4. Loop Back (The Cycle)\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 5. Compile with Memory\n",
    "# This is the key \"Stateful\" component.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# ==========================================\n",
    "# SECTION 5: EXECUTION (The Interview Demo)\n",
    "# ==========================================\n",
    "\n",
    "def print_stream(stream_event):\n",
    "    \"\"\"Helper to pretty-print the events typesafely.\"\"\"\n",
    "    for key, value in stream_event.items():\n",
    "        if \"messages\" in value:\n",
    "            last_msg = value[\"messages\"][-1]\n",
    "            \n",
    "            # TYPE SAFETY: Check strictly what kind of message this is\n",
    "            if isinstance(last_msg, AIMessage):\n",
    "                if last_msg.tool_calls:\n",
    "                    names = [t['name'] for t in last_msg.tool_calls]\n",
    "                    print(f\"üõ†Ô∏è  Thinking: I need to use tools: {names}\")\n",
    "                else:\n",
    "                    print(f\"ü§ñ Agent: {last_msg.content}\")\n",
    "            \n",
    "            elif isinstance(last_msg, ToolMessage):\n",
    "                # Truncate output for cleanliness\n",
    "                content = str(last_msg.content)\n",
    "                print(f\"‚ö° Tool Output: {content[:80]}...\")\n",
    "\n",
    "def main():\n",
    "    # A unique thread ID simulates a specific user session.\n",
    "    # The agent uses this ID to look up previous messages in MemorySaver.\n",
    "    thread_id = \"google_interview_session_v1\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    print(f\"\\n=== SESSION START (Thread ID: {thread_id}) ===\\n\")\n",
    "\n",
    "    # --- TURN 1 ---\n",
    "    print(\"--- Turn 1: Initial Research Request ---\")\n",
    "    user_input_1 = \"Search for what is trending in AI today.\"\n",
    "    print(f\"User: {user_input_1}\")\n",
    "    \n",
    "    inputs_1 = {\"messages\": [HumanMessage(content=user_input_1)]}\n",
    "    \n",
    "    try:\n",
    "        # Recursion limit safeguards against infinite loops\n",
    "        for event in app.stream(inputs_1, config=config):\n",
    "            print_stream(event)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # --- TURN 2 (Testing State) ---\n",
    "    print(\"\\n--- Turn 2: Follow-up (Testing Memory) ---\")\n",
    "    # Note: We do NOT repeat the topic. The agent must recall it from state.\n",
    "    user_input_2 = \"Write a kids transcript about that.\"\n",
    "    print(f\"User: {user_input_2}\")\n",
    "    \n",
    "    inputs_2 = {\"messages\": [HumanMessage(content=user_input_2)]}\n",
    "    \n",
    "    try:\n",
    "        for event in app.stream(inputs_2, config=config):\n",
    "            print_stream(event)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    print(\"\\n=== SESSION END ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "._venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
