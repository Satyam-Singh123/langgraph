{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_react_agent.py\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import traceback\n",
    "from typing import Any, Optional, Dict, Callable\n",
    "\n",
    "# Disable LangSmith logging (prevents unwanted 403 errors)\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import requests\n",
    "\n",
    "# Watsonx imports\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "# LangGraph + messages\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# typing for AgentState\n",
    "from typing import Annotated, List, TypedDict, Optional\n",
    "import operator\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Agent State\n",
    "# -----------------------------------------------------------\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    action_name: Optional[str]\n",
    "    action_input: Optional[dict]\n",
    "    raw_output: Optional[str]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ReAct Prompt\n",
    "# -----------------------------------------------------------\n",
    "REACT_PROMPT = \"\"\"\n",
    "You are a specialized agent. Your goal is to answer the user's request.\n",
    "You have access to the following tool: {tool_name} with the following description: {tool_description}\n",
    "\n",
    "You must respond in one of two formats:\n",
    "\n",
    "1. Final Answer:\n",
    "Thought: I have enough information to answer the user.\n",
    "Action: Final Answer\n",
    "Action Input: The final answer goes here.\n",
    "\n",
    "2. Tool Call:\n",
    "Thought: I need to use the tool to find the answer.\n",
    "Action: {tool_name}\n",
    "Action Input: {{\"query\": \"the search term goes here\"}}\n",
    "\n",
    "Begin.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Watsonx LLM Client (Llama 3.3 70B instruct)\n",
    "# -----------------------------------------------------------\n",
    "def get_watsonx_llm(\n",
    "    model_id: str = \"meta-llama/llama-3-3-70b-instruct\",\n",
    "    max_new_tokens: int = 512,\n",
    "    temperature: float = 0.2,\n",
    ") -> ModelInference:\n",
    "\n",
    "    api_key = os.getenv(\"WATSONX_API_KEY\")\n",
    "    url = os.getenv(\"WATSONX_URL\")\n",
    "    project_id = os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "    if not api_key or not url or not project_id:\n",
    "        raise RuntimeError(\"Missing Watsonx environment variables.\")\n",
    "\n",
    "    creds = Credentials(api_key=api_key, url=url)\n",
    "    client = APIClient(credentials=creds)\n",
    "\n",
    "    return ModelInference(\n",
    "        model_id=model_id,\n",
    "        api_client=client,\n",
    "        project_id=project_id,\n",
    "        params={\n",
    "            GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "            GenParams.TEMPERATURE: temperature,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# REAL Tavily Search Tool\n",
    "# -----------------------------------------------------------\n",
    "def tavily_search(query: str, top_k: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Real Tavily search call.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    endpoint = os.getenv(\"TAVILY_ENDPOINT\")\n",
    "\n",
    "    if not api_key or not endpoint:\n",
    "        raise RuntimeError(\"Missing Tavily environment variables.\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\"query\": query, \"top_k\": top_k}\n",
    "    resp = requests.post(endpoint, headers=headers, json=payload, timeout=25)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Transcript Generator (Watsonx Llama)\n",
    "# -----------------------------------------------------------\n",
    "def generate_transcript(topic: str, llm: Optional[ModelInference] = None) -> str:\n",
    "    if llm is None:\n",
    "        llm = get_watsonx_llm()\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Generate a clear, spoken-style transcript for a YouTube trending video for kids about:\n",
    "\"{topic}\"\n",
    "\n",
    "Requirements:\n",
    "- 400–700 words\n",
    "- Conversational tone\n",
    "- Clear structure: hook → points → ending\n",
    "- No directions or metadata, only spoken words\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.generate_text(prompt)\n",
    "\n",
    "    if isinstance(response, dict) and \"results\" in response:\n",
    "        return response[\"results\"][0].get(\"generated_text\", \"\")\n",
    "    elif isinstance(response, str):\n",
    "        return response\n",
    "    return str(response)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# LLM Node (ReAct)\n",
    "# -----------------------------------------------------------\n",
    "def react_llm_node(\n",
    "    state: AgentState,\n",
    "    tool_name: str,\n",
    "    tool_description: str,\n",
    "    model_id: str = \"meta-llama/llama-3-3-70b-instruct\",\n",
    "):\n",
    "    conversation = \"\"\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            conversation += f\"User: {msg.content}\\n\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            conversation += f\"Assistant: {msg.content}\\n\"\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            conversation += f\"Tool: {msg.content}\\n\"\n",
    "\n",
    "    prompt = REACT_PROMPT.format(\n",
    "        tool_name=tool_name,\n",
    "        tool_description=tool_description\n",
    "    ) + \"\\nConversation:\\n\" + conversation\n",
    "\n",
    "    llm = get_watsonx_llm(model_id=model_id)\n",
    "    response = llm.generate_text(prompt)\n",
    "\n",
    "    if isinstance(response, dict) and \"results\" in response:\n",
    "        raw_text = response[\"results\"][0][\"generated_text\"]\n",
    "    elif isinstance(response, str):\n",
    "        raw_text = response\n",
    "    else:\n",
    "        raw_text = str(response)\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=raw_text)], \"raw_output\": raw_text}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Parse & Decide Router (ReAct)\n",
    "# -----------------------------------------------------------\n",
    "def parse_and_decide(state: AgentState):\n",
    "    last = state[\"messages\"][-1]\n",
    "    if not isinstance(last, AIMessage):\n",
    "        return {}\n",
    "\n",
    "    text = last.content or \"\"\n",
    "\n",
    "    print(\"\\n--- Parsing ReAct Output ---\")\n",
    "    print(text[:600], \"...\\n\")\n",
    "\n",
    "    m = re.search(\n",
    "        r\"Action:\\s*(.+?)\\s*\\nAction Input:\\s*(.+)\",\n",
    "        text,\n",
    "        re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "    if not m:\n",
    "        return {\"action_name\": None, \"action_input\": None}\n",
    "\n",
    "    action_name = m.group(1).strip()\n",
    "    action_input_raw = m.group(2).strip()\n",
    "\n",
    "    try:\n",
    "        action_input = json.loads(action_input_raw)\n",
    "    except:\n",
    "        try:\n",
    "            action_input = ast.literal_eval(action_input_raw)\n",
    "        except:\n",
    "            action_input = {\"raw\": action_input_raw}\n",
    "\n",
    "    if action_name.lower() == \"final answer\":\n",
    "        return {\"action_name\": None, \"action_input\": action_input}\n",
    "\n",
    "    return {\n",
    "        \"action_name\": action_name,\n",
    "        \"action_input\": action_input,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# TOOL EXECUTOR with correct ToolMessage(tool_call_id=...)\n",
    "# -----------------------------------------------------------\n",
    "TOOLS: Dict[str, Callable[..., Any]] = {\n",
    "    \"tavily_search\": tavily_search,\n",
    "}\n",
    "\n",
    "def tool_executor_node(state: AgentState):\n",
    "    name = state.get(\"action_name\")\n",
    "    data = state.get(\"action_input\") or {}\n",
    "\n",
    "    if not name:\n",
    "        return {}\n",
    "\n",
    "    tool_fn = TOOLS.get(name)\n",
    "    if not tool_fn:\n",
    "        result_text = f\"Unknown tool: {name}\"\n",
    "    else:\n",
    "        try:\n",
    "            if isinstance(data, dict) and \"query\" in data:\n",
    "                result = tool_fn(query=data[\"query\"], **{k: v for k, v in data.items() if k != \"query\"})\n",
    "            else:\n",
    "                result = tool_fn(**data)\n",
    "            result_text = json.dumps(result, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            result_text = json.dumps({\"error\": str(e), \"trace\": traceback.format_exc()})\n",
    "\n",
    "    # FIX: ToolMessage requires tool_call_id\n",
    "    tool_msg = ToolMessage(\n",
    "        content=result_text,\n",
    "        tool_call_id=name\n",
    "    )\n",
    "\n",
    "    return {\"messages\": [tool_msg], \"action_name\": None, \"action_input\": None}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Extract Final Answer\n",
    "# -----------------------------------------------------------\n",
    "# def extract_final_answer(state: AgentState) -> Optional[str]:\n",
    "#     for msg in reversed(state[\"messages\"]):\n",
    "#         if isinstance(msg, AIMessage) and \"Final Answer\" in msg.content:\n",
    "#             m = re.search(r\"Action Input:\\s*(.+)$\", msg.content, re.S)\n",
    "#             if m:\n",
    "#                 return m.group(1).strip()\n",
    "#             return msg.content\n",
    "#     return None\n",
    "\n",
    "\n",
    "def extract_final_answer(state):\n",
    "    # Access the list of messages inside state[\"messages\"]\n",
    "    messages = state[\"messages\"][\"messages\"]\n",
    "\n",
    "    for msg in reversed(messages):\n",
    "        if isinstance(msg, AIMessage) and \"Final Answer\" in msg.content:\n",
    "            # Extract everything after \"Final Answer\"\n",
    "            m = re.search(r\"Final Answer\\s*(.*)\", msg.content, re.S)\n",
    "            if m:\n",
    "                return m.group(1).strip()\n",
    "            return msg.content.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Build LangGraph\n",
    "# -----------------------------------------------------------\n",
    "def build_graph():\n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    graph.add_node(\n",
    "        \"react_llm\",\n",
    "        lambda s: react_llm_node(\n",
    "            s,\n",
    "            tool_name=\"tavily_search\",\n",
    "            tool_description=\"Searches the internet and returns relevant info via Tavily.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    graph.add_node(\"router\", parse_and_decide)\n",
    "    graph.add_node(\"tool_executor\", tool_executor_node)\n",
    "\n",
    "    graph.add_edge(START, \"react_llm\")\n",
    "    graph.add_edge(\"react_llm\", \"router\")\n",
    "\n",
    "    def route_fn(state: AgentState):\n",
    "        return \"end\" if state.get(\"action_name\") is None else \"tool\"\n",
    "\n",
    "    graph.add_conditional_edges(\n",
    "        \"router\",\n",
    "        route_fn,\n",
    "        {\"end\": END, \"tool\": \"tool_executor\"}\n",
    "    )\n",
    "\n",
    "    graph.add_edge(\"tool_executor\", \"react_llm\")\n",
    "    return graph\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# MAIN\n",
    "# -----------------------------------------------------------\n",
    "def main():\n",
    "    graph = build_graph()\n",
    "    app = graph.compile()\n",
    "\n",
    "    user_query = input(\"Enter your query/topic: \").strip()\n",
    "\n",
    "    state: AgentState = {\n",
    "        \"messages\": [HumanMessage(content=user_query)],\n",
    "        \"action_name\": None,\n",
    "        \"action_input\": None,\n",
    "        \"raw_output\": None,\n",
    "    }\n",
    "\n",
    "    result = app.invoke(state)\n",
    "\n",
    "    final = extract_final_answer(result)\n",
    "    if final:\n",
    "        print(\"\\n==== Final Answer ====\\n\")\n",
    "        print(final)\n",
    "\n",
    "        print(\"\\n==== Generating Transcript ====\\n\")\n",
    "        # topic = user_query\n",
    "        transcript = generate_transcript(final)\n",
    "        print(transcript)\n",
    "    else:\n",
    "        print(\"\\n==== No Final Answer Found ====\\n\")\n",
    "        for m in result[\"messages\"]:\n",
    "            print(type(m).__name__, \":\", m.content)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46cdd88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "._venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
