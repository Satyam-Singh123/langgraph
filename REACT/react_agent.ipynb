{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5ee64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @langchain_api: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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b0f44",
   "metadata": {},
   "source": [
    "We create a prompt that explicitly instructs the LLM to output a structured sequence of Thought, Action, and Action Input.\n",
    "\n",
    "\n",
    "A. Defining the Agent State (The Shared Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import operator\n",
    "from typing import Annotated, List, TypedDict, Optional\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# --- AgentState Definition ---\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the agent's work. It is the single source of truth\n",
    "    passed between all nodes in the graph.\n",
    "    \"\"\"\n",
    "    # 1. Conversation History (Reducer: Append messages)\n",
    "    messages: Annotated[List[BaseMessage], operator.add] \n",
    "    '''Key Concept: We use Annotated with operator.add for the messages list. \n",
    "    This is the reducer function that tells LangGraph: \n",
    "    \"When a node returns new messages, append them to the existing list, don't overwrite the history.\n",
    "    \"'''\n",
    "    \n",
    "    # 2. Tool Action (Used by the Conditional Edge/Tool Executor)\n",
    "    # The name of the tool the agent decided to use (e.g., 'search_tool')\n",
    "    action_name: Optional[str] \n",
    "    \n",
    "    # 3. Tool Input (The arguments for the tool)\n",
    "    # The JSON string or dictionary passed to the tool\n",
    "    action_input: Optional[dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63510d69",
   "metadata": {},
   "source": [
    "B. ReAct Prompt Template\n",
    "\n",
    "Since we are using watsonx.ai models that don't natively support tool calling, we must use a system prompt to enforce the ReAct structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea958fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827481ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_PROMPT = \"\"\"\n",
    "You are a specialized agent. Your goal is to answer the user's request.\n",
    "You have access to the following tool: {tool_name} with the following description: {tool_description}\n",
    "\n",
    "You must respond in one of two formats:\n",
    "\n",
    "1. Final Answer:\n",
    "Thought: I have enough information to answer the user.\n",
    "Action: Final Answer\n",
    "Action Input: The final answer goes here.\n",
    "\n",
    "2. Tool Call:\n",
    "Thought: I need to use the tool to find the answer.\n",
    "Action: {tool_name}\n",
    "Action Input: {{\"query\": \"the search term goes here\"}}\n",
    "\n",
    "Begin.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c331085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d241b734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7345f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watsonx_llm(\n",
    "        model_id=\"meta-llama/llama-3-2-11b-instruct\",\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.2\n",
    "    ) -> ModelInference:\n",
    "\n",
    "    creds = Credentials(\n",
    "        api_key = os.getenv(\"WATSONX_API_KEY\"),\n",
    "        url = os.getenv(\"WATSONX_URL\") \n",
    "    )\n",
    "\n",
    "    client = APIClient(credentials=creds)\n",
    "\n",
    "    return ModelInference(\n",
    "        model_id=model_id,\n",
    "        api_client=client,\n",
    "        project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "        params={\n",
    "            GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "            GenParams.TEMPERATURE: temperature,\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8b49617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def react_llm_node(state: AgentState, tool_name: str, tool_description: str):\n",
    "    \"\"\"\n",
    "    LangGraph node that:\n",
    "    1. Builds the ReAct prompt using conversation history\n",
    "    2. Sends it to Watsonx LLaMA (ModelInference)\n",
    "    3. Produces an AIMessage containing the model output\n",
    "    4. Updates the 'messages' list in AgentState\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---- 1. Build the prompt by combining all previous messages ----\n",
    "    conversation = \"\"\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            conversation += f\"User: {msg.content}\\n\"\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            conversation += f\"Assistant: {msg.content}\\n\"\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            conversation += f\"Tool: {msg.content}\\n\"\n",
    "\n",
    "    # Include your ReAct base template\n",
    "    prompt = REACT_PROMPT.format(\n",
    "        tool_name=tool_name,\n",
    "        tool_description=tool_description\n",
    "    ) + \"\\n\" + conversation + \"\\n\"\n",
    "\n",
    "    # ---- 2. Run the LLaMA model through Watsonx ----\n",
    "    llm = get_watsonx_llm()\n",
    "\n",
    "    response = llm.generate_text(prompt)\n",
    "    raw_text = response[\"results\"][0][\"generated_text\"]\n",
    "\n",
    "    # ---- 3. Wrap into AIMessage ----\n",
    "    ai_msg = AIMessage(content=raw_text)\n",
    "\n",
    "    # ---- 4. Return update for LangGraph state ----\n",
    "    return {\"messages\": [ai_msg]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d107724",
   "metadata": {},
   "source": [
    "C. The ReAct Parsing Node (The Crucial Step)\n",
    "\n",
    "This function takes the raw text output from the watsonx.ai model and uses regular expressions to extract the Action and Action Input needed for the graph logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5e303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2280b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_and_decide(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Parses the last AI message for ReAct structure and updates the state.\n",
    "    This also acts as the router for the conditional edge.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Ensure we are parsing an AI message\n",
    "    if not isinstance(last_message, AIMessage):\n",
    "        return {} # No update if it's not the LLM's output\n",
    "\n",
    "    text = last_message.content\n",
    "    print(f\"\\n--- Parsing LLM Output: {text[:50]}... ---\")\n",
    "\n",
    "    # Regex to find the Action and Action Input\n",
    "    action_match = re.search(r\"Action: (.+?)\\nAction Input: (.+)\", text, re.DOTALL)\n",
    "\n",
    "    if action_match:\n",
    "        action_name = action_match.group(1).strip()\n",
    "        action_input_str = action_match.group(2).strip()\n",
    "        \n",
    "        # Try to parse Action Input as JSON\n",
    "        try:\n",
    "            # We assume Action Input is a simple JSON string {\"query\": \"...\"}\n",
    "            action_input = eval(action_input_str) \n",
    "        except:\n",
    "            print(\"Warning: Could not parse Action Input. Assuming final answer.\")\n",
    "            action_name = \"Final Answer\"\n",
    "            action_input = None\n",
    "\n",
    "        if action_name == \"Final Answer\":\n",
    "            # The agent decided to finish. Clear action tracking fields.\n",
    "            return {\"action_name\": None, \"action_input\": None}\n",
    "        else:\n",
    "            # The agent decided to use a tool. Update state for the next node.\n",
    "            return {\"action_name\": action_name, \"action_input\": action_input}\n",
    "    \n",
    "    # If no clear ReAct structure is found, assume the LLM returned a final answer.\n",
    "    return {\"action_name\": None, \"action_input\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e4d4d",
   "metadata": {},
   "source": [
    "D. The Conditional Edge Logic (The Router)\n",
    "\n",
    "The router is now trivial because the parse_and_decide node already updated the state with the action name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6d1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Checks the state to see if a tool call is pending or if the agent is done.\"\"\"\n",
    "    \n",
    "    # If action_name is set by the parser, we need to call the tool\n",
    "    if state.get(\"action_name\"):\n",
    "        return \"call_tool\"\n",
    "    else:\n",
    "        # Otherwise, the parser decided the LLM gave the final answer\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0268340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_executor_node(state: AgentState, tools: dict):\n",
    "    tool_name = state[\"action_name\"]\n",
    "    action_input = state[\"action_input\"]\n",
    "\n",
    "    tool_fn = tools.get(tool_name)\n",
    "    result = tool_fn(**action_input)\n",
    "\n",
    "    tool_msg = ToolMessage(content=str(result), name=tool_name)\n",
    "    return {\"messages\": [tool_msg], \"action_name\": None, \"action_input\": None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab47f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(state: AgentState):\n",
    "    if state[\"action_name\"] is None:\n",
    "        return \"finish\"\n",
    "    return \"tool_executor\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b05ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Nodes\n",
    "graph.add_node(\n",
    "    \"react_llm\",\n",
    "    lambda state: react_llm_node(\n",
    "        state,\n",
    "        tool_name=\"search_tool\",\n",
    "        tool_description=\"Searches indexed documents.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "graph.add_node(\"router\", parse_and_decide)\n",
    "graph.add_node(\"tool_executor\", tool_executor_node)\n",
    "\n",
    "# Edges\n",
    "graph.add_edge(START, \"react_llm\")\n",
    "graph.add_edge(\"react_llm\", \"router\")\n",
    "\n",
    "def route(state: AgentState):\n",
    "    return \"end\" if state[\"action_name\"] is None else \"tool\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    route,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"tool\": \"tool_executor\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"tool_executor\", \"react_llm\")\n",
    "\n",
    "# Compile\n",
    "app = graph.compile()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "._venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
